{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjyhWa5xAYEJ"
      },
      "source": [
        "## T-GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxLSzFTW1E1h",
        "outputId": "d8d613ae-3343-4a2e-fdcb-0de0f50ed421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install torch geometric -- for pyg\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install -q torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2F3E2XB1H4m",
        "outputId": "b8674119-58e9-448a-8fd2-578ec5d87c79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n",
            "  import torch_geometric.typing\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_vaRUKqQa8Q",
        "outputId": "2e7ff1e5-cc51-41d2-b83e-ba394ee5daba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "MyDrive  Shareddrives\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "75Qlu8TaMTGR"
      },
      "outputs": [],
      "source": [
        "class SkyNetDataset(InMemoryDataset):\n",
        "  def __init__(self, root, transform=None, pre_transform=None):\n",
        "    super().__init__(root, transform, pre_transform)\n",
        "    self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return []\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return ['flight_graphs.pt']\n",
        "\n",
        "  def process(self):\n",
        "    pass\n",
        "\n",
        "dataset = SkyNetDataset(root=\"/content/drive/Shareddrives/CS_224W_Project/data/data/skynet_clean_graphs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3Sy9Zl_4DVu8"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def calculate_laplacian_with_self_loop(matrix: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Normalized Laplacian with self-loops:\n",
        "        L = D^{-1/2} (A + I) D^{-1/2}\n",
        "    \"\"\"\n",
        "    device = matrix.device\n",
        "    matrix = matrix + torch.eye(matrix.size(0), device=device)\n",
        "    row_sum = matrix.sum(1)\n",
        "    d_inv_sqrt = torch.pow(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n",
        "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
        "    normalized_laplacian = (\n",
        "        matrix.matmul(d_mat_inv_sqrt).transpose(0, 1).matmul(d_mat_inv_sqrt)\n",
        "    )\n",
        "    return normalized_laplacian\n",
        "\n",
        "\n",
        "class TGraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph convolution used inside the T-GCN gates, but with multi-dimensional\n",
        "    node features.\n",
        "\n",
        "    Args:\n",
        "        adj:           (N, N) adjacency matrix\n",
        "        input_dim:     F  (node feature dim)\n",
        "        num_gru_units: H  (hidden dim)\n",
        "        output_dim:    O\n",
        "\n",
        "    Inputs:\n",
        "        inputs:       (B, N, F)\n",
        "        hidden_state: (B, N*H) or (B, N, H)\n",
        "\n",
        "    Output:\n",
        "        (B, N*O)\n",
        "    \"\"\"\n",
        "    def __init__(self, adj, input_dim: int, num_gru_units: int,\n",
        "                 output_dim: int, bias: float = 0.0):\n",
        "        super().__init__()\n",
        "        self._input_dim = input_dim\n",
        "        self._num_gru_units = num_gru_units\n",
        "        self._output_dim = output_dim\n",
        "        self._bias_init_value = bias\n",
        "\n",
        "        self.register_buffer(\n",
        "            \"laplacian\",\n",
        "            calculate_laplacian_with_self_loop(torch.as_tensor(adj, dtype=torch.float32))\n",
        "        )\n",
        "        self.weights = nn.Parameter(\n",
        "            torch.FloatTensor(self._input_dim + self._num_gru_units, self._output_dim)\n",
        "        )\n",
        "        self.biases = nn.Parameter(torch.FloatTensor(self._output_dim))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weights)\n",
        "        nn.init.constant_(self.biases, self._bias_init_value)\n",
        "\n",
        "    def forward(self, inputs, hidden_state):\n",
        "        # inputs: (B, N, F)\n",
        "        batch_size, num_nodes, in_dim = inputs.shape\n",
        "        assert in_dim == self._input_dim\n",
        "\n",
        "        # hidden_state -> (B, N, H)\n",
        "        if hidden_state.dim() == 2:\n",
        "            hidden_state = hidden_state.view(batch_size, num_nodes, self._num_gru_units)\n",
        "        elif hidden_state.dim() == 3:\n",
        "            assert hidden_state.shape[2] == self._num_gru_units\n",
        "        else:\n",
        "            raise ValueError(f\"hidden_state must have dim 2 or 3, got {hidden_state.dim()}\")\n",
        "\n",
        "        # concat node features & hidden states\n",
        "        concat = torch.cat((inputs, hidden_state), dim=2)   # (B, N, F+H)\n",
        "        B, N, C = concat.shape\n",
        "\n",
        "        # A * [x, h]\n",
        "        concat = concat.permute(1, 2, 0).reshape(N, C * B)  # (N, C*B)\n",
        "        a_times_concat = self.laplacian @ concat            # (N, C*B)\n",
        "        a_times_concat = a_times_concat.view(N, C, B).permute(2, 0, 1)  # (B, N, C)\n",
        "        a_times_concat = a_times_concat.reshape(B * N, C)   # (B*N, C)\n",
        "\n",
        "        outputs = a_times_concat @ self.weights + self.biases   # (B*N, O)\n",
        "        outputs = outputs.view(B, N, self._output_dim)          # (B, N, O)\n",
        "        outputs = outputs.reshape(B, N * self._output_dim)      # (B, N*O)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class TGCNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    One T-GCN cell (one time step).\n",
        "\n",
        "    Args:\n",
        "        adj:        (N, N)\n",
        "        input_dim:  F\n",
        "        hidden_dim: H\n",
        "    \"\"\"\n",
        "    def __init__(self, adj, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self._input_dim = input_dim\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self.register_buffer(\"adj\", torch.as_tensor(adj, dtype=torch.float32))\n",
        "\n",
        "        # gate [r, u]\n",
        "        self.graph_conv1 = TGraphConvolution(\n",
        "            self.adj, self._input_dim, self._hidden_dim, self._hidden_dim * 2, bias=1.0\n",
        "        )\n",
        "        # candidate c\n",
        "        self.graph_conv2 = TGraphConvolution(\n",
        "            self.adj, self._input_dim, self._hidden_dim, self._hidden_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, hidden_state):\n",
        "        \"\"\"\n",
        "        inputs:       (B, N, F)\n",
        "        hidden_state: (B, N*H)\n",
        "        returns:      (B, N*H), (B, N*H)\n",
        "        \"\"\"\n",
        "        # [r, u] = sigmoid(A[x, h]W + b)  -> (B, N*2H)\n",
        "        concatenation = torch.sigmoid(self.graph_conv1(inputs, hidden_state))\n",
        "        r, u = torch.chunk(concatenation, chunks=2, dim=1)  # each (B, N*H)\n",
        "\n",
        "        # candidate c = tanh(A[x, (r * h)]W + b)\n",
        "        c = torch.tanh(self.graph_conv2(inputs, r * hidden_state))  # (B, N*H)\n",
        "\n",
        "        # new hidden: u * h + (1 - u) * c\n",
        "        new_hidden_state = u * hidden_state + (1.0 - u) * c\n",
        "        return new_hidden_state, new_hidden_state\n",
        "\n",
        "\n",
        "class TGCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Temporal Graph Convolutional Network over airports.\n",
        "\n",
        "    Inputs:\n",
        "        inputs: (B, T, N, F)  – batch, time, num_nodes, node_feat_dim\n",
        "    Outputs:\n",
        "        (B, N, H) – node embeddings at final time step\n",
        "    \"\"\"\n",
        "    def __init__(self, adj, input_dim: int, hidden_dim: int, **kwargs):\n",
        "        super().__init__()\n",
        "        self._num_nodes = adj.shape[0]\n",
        "        self._input_dim = input_dim\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self.register_buffer(\"adj\", torch.as_tensor(adj, dtype=torch.float32))\n",
        "        self.tgcn_cell = TGCNCell(self.adj, self._input_dim, self._hidden_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size, seq_len, num_nodes, in_dim = inputs.shape\n",
        "        assert num_nodes == self._num_nodes\n",
        "        assert in_dim == self._input_dim\n",
        "\n",
        "        hidden_state = torch.zeros(\n",
        "            batch_size,\n",
        "            num_nodes * self._hidden_dim,\n",
        "            device=inputs.device,\n",
        "            dtype=inputs.dtype,\n",
        "        )\n",
        "\n",
        "        output = None\n",
        "        for t in range(seq_len):\n",
        "            x_t = inputs[:, t, :, :]          # (B, N, F)\n",
        "            output, hidden_state = self.tgcn_cell(x_t, hidden_state)\n",
        "\n",
        "        # output: (B, N*H) -> (B, N, H)\n",
        "        last_output = output.view(batch_size, num_nodes, self._hidden_dim)\n",
        "        return last_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MorCZaAFAYEL"
      },
      "outputs": [],
      "source": [
        "# Edge-level T-GCN delay model\n",
        "class TGCNDelayModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full model:\n",
        "      - T-GCN over airport nodes: node weather+geo → time-aware embeddings\n",
        "      - MLP over edges: [h_src || h_dst || edge_features] → delay (minutes).\n",
        "\n",
        "    We use regression (MSE) here.\n",
        "    \"\"\"\n",
        "    def __init__(self, adj, node_feat_dim: int, edge_feat_dim: int,\n",
        "                 hidden_dim: int = 64, edge_hidden_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.num_nodes = adj.shape[0]\n",
        "\n",
        "        # Temporal graph encoder over airports\n",
        "        self.tgcn = TGCN(adj, node_feat_dim, hidden_dim)\n",
        "\n",
        "        # Edge prediction head\n",
        "        in_dim = 2 * hidden_dim + edge_feat_dim\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, edge_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(edge_hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, node_seq, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        node_seq:   (B, T, N, F_node)\n",
        "        edge_index: (B, 2, E)  (we assume B=1 with DataLoader(batch_size=1))\n",
        "        edge_attr:  (B, E, F_edge)\n",
        "\n",
        "        Returns:\n",
        "            preds: (B, E)  – predicted delay (minutes, or normalized minutes)\n",
        "        \"\"\"\n",
        "        batch_size = node_seq.shape[0]\n",
        "        assert batch_size == 1, \"TGCNDelayModel currently assumes batch_size=1.\"\n",
        "\n",
        "        # Normalise edge_index shape: (B, 2, E) -> (2, E) (since B=1)\n",
        "        if edge_index.dim() == 3:\n",
        "            edge_index = edge_index[0]\n",
        "        src, dst = edge_index         # (E,), (E,)\n",
        "\n",
        "        # T-GCN over nodes\n",
        "        node_embeddings = self.tgcn(node_seq)   # (B=1, N, H)\n",
        "\n",
        "        # Gather embeddings for origin & destination\n",
        "        h_src = node_embeddings[:, src, :]      # (B=1, E, H)\n",
        "        h_dst = node_embeddings[:, dst, :]      # (B=1, E, H)\n",
        "\n",
        "        # Normalise edge_attr shape\n",
        "        if edge_attr.dim() == 2:\n",
        "            edge_attr_expanded = edge_attr.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        else:\n",
        "            edge_attr_expanded = edge_attr      # (B=1, E, F_edge)\n",
        "\n",
        "        edge_inputs = torch.cat([h_src, h_dst, edge_attr_expanded], dim=-1)  # (B=1, E, 2H+F_e)\n",
        "        preds = self.edge_mlp(edge_inputs).squeeze(-1)                       # (B=1, E)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sz9qnTkCRRo",
        "outputId": "d3020269-748a-408d-e705-d0ea10150728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjacency shape: torch.Size([107, 107])\n"
          ]
        }
      ],
      "source": [
        "# Static airport adjacency from PyG dataset\n",
        "def build_adj_from_dataset(pyg_dataset):\n",
        "    \"\"\"\n",
        "    Build a static adjacency over airports by combining edges\n",
        "    from all snapshots in the dataset.\n",
        "\n",
        "    Assumes that:\n",
        "      - All graphs share the same num_nodes\n",
        "      - Node indices are consistent across time.\n",
        "    \"\"\"\n",
        "    num_nodes = pyg_dataset[0].num_nodes\n",
        "    adj = torch.zeros(num_nodes, num_nodes, dtype=torch.float32)\n",
        "\n",
        "    for data in pyg_dataset:\n",
        "        ei = data.edge_index\n",
        "        src = ei[0]\n",
        "        dst = ei[1]\n",
        "        adj[src, dst] = 1.0\n",
        "        adj[dst, src] = 1.0  # undirected for Laplacian\n",
        "\n",
        "    return adj\n",
        "\n",
        "adj = build_adj_from_dataset(dataset)\n",
        "print(\"Adjacency shape:\", adj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3tb-ZpMfCTeh"
      },
      "outputs": [],
      "source": [
        "# Sequence dataset for T-GCN\n",
        "class SequenceSkyNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps your PyG SkyNetDataset into sequences for T-GCN.\n",
        "\n",
        "    Each item:\n",
        "      node_seq:   (T, N, F_node)   – node features for T consecutive time steps\n",
        "      edge_index: (2, E)           – edges of the last time step\n",
        "      edge_attr:  (E, F_edge)      – edge features of the last time step\n",
        "      y:          (E,)             – departure delay labels of the last time step\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset, history_len=4, require_edges=True):\n",
        "        self.base = base_dataset\n",
        "        self.history_len = history_len\n",
        "        self.require_edges = require_edges\n",
        "\n",
        "        # Sort indices by time or block_idx if available\n",
        "        indices = list(range(len(base_dataset)))\n",
        "        if hasattr(base_dataset[0], \"block_idx\"):\n",
        "            indices = sorted(indices, key=lambda i: int(base_dataset[i].block_idx))\n",
        "\n",
        "        self.sorted_indices = indices\n",
        "\n",
        "        # Build list of valid target positions\n",
        "        valid_positions = []\n",
        "        for pos in range(history_len - 1, len(self.sorted_indices)):\n",
        "            idx = self.sorted_indices[pos]\n",
        "            data = self.base[idx]\n",
        "            if require_edges and data.edge_index.size(1) == 0:\n",
        "                continue\n",
        "            valid_positions.append(pos)\n",
        "\n",
        "        self.valid_positions = valid_positions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_positions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        idx indexes into valid_positions, not directly into base_dataset.\n",
        "        \"\"\"\n",
        "        pos = self.valid_positions[idx]\n",
        "        # history positions for this target\n",
        "        hist_positions = self.sorted_indices[pos - self.history_len + 1 : pos + 1]\n",
        "\n",
        "        graphs = [self.base[i] for i in hist_positions]\n",
        "\n",
        "        # Node sequence: (T, N, F_node)\n",
        "        node_seq = torch.stack([g.x for g in graphs], dim=0)\n",
        "\n",
        "        # Last graph provides edges, edge features, labels\n",
        "        target = graphs[-1]\n",
        "        edge_index = target.edge_index\n",
        "        edge_attr = target.edge_attr\n",
        "        y = target.y\n",
        "\n",
        "        return node_seq, edge_index, edge_attr, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W7P3iQY47KgT"
      },
      "outputs": [],
      "source": [
        "def build_adj_from_seq_subset(seq_dataset, ds_indices):\n",
        "    \"\"\"\n",
        "    Build adjacency using only the graphs that appear in the history windows\n",
        "    for the given sequence indices (ds_indices are indices into seq_dataset).\n",
        "    \"\"\"\n",
        "    base = seq_dataset.base\n",
        "    num_nodes = base[0].num_nodes\n",
        "    adj = torch.zeros(num_nodes, num_nodes, dtype=torch.float32)\n",
        "\n",
        "    for ds_idx in ds_indices:\n",
        "        pos = seq_dataset.valid_positions[ds_idx]\n",
        "        # include the whole history window used for this sequence\n",
        "        hist_positions = seq_dataset.sorted_indices[\n",
        "            pos - seq_dataset.history_len + 1 : pos + 1\n",
        "        ]\n",
        "        for idx in hist_positions:\n",
        "            g = base[idx]\n",
        "            ei = g.edge_index\n",
        "            src, dst = ei[0], ei[1]\n",
        "            adj[src, dst] = 1.0\n",
        "            adj[dst, src] = 1.0  # undirected\n",
        "    return adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd1zvUIOEsBu",
        "outputId": "add49334-9b10-4fc8-e546-f07123675c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1019 219 219\n"
          ]
        }
      ],
      "source": [
        "# Build sequence dataset\n",
        "history_len = 4  # 4 * 6h = 24h of history\n",
        "seq_dataset = SequenceSkyNetDataset(dataset, history_len=history_len)\n",
        "\n",
        "num_samples = len(seq_dataset)\n",
        "train_end = int(0.7 * num_samples)\n",
        "val_end   = int(0.85 * num_samples)\n",
        "\n",
        "train_set = torch.utils.data.Subset(seq_dataset, range(0, train_end))\n",
        "val_set   = torch.utils.data.Subset(seq_dataset, range(train_end, val_end))\n",
        "test_set  = torch.utils.data.Subset(seq_dataset, range(val_end, num_samples))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=1, shuffle=False)\n",
        "test_loader  = DataLoader(test_set,  batch_size=1, shuffle=False)\n",
        "\n",
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3l7uAc7E_dQc"
      },
      "outputs": [],
      "source": [
        "# Instantiate T-GCN model\n",
        "sample_graph = dataset[0]\n",
        "node_feat_dim = sample_graph.x.size(1)         # should be 9: [lat, lon, 7 weather]\n",
        "edge_feat_dim = sample_graph.edge_attr.size(1) # should be 13: flight features\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TGCNDelayModel(\n",
        "    adj=adj,\n",
        "    node_feat_dim=node_feat_dim,\n",
        "    edge_feat_dim=edge_feat_dim,\n",
        "    hidden_dim=64,\n",
        "    edge_hidden_dim=64,\n",
        "    # task=\"regression\"\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "loss_fn = nn.MSELoss()  # MSE in minutes^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6Fd29NI8_j2D"
      },
      "outputs": [],
      "source": [
        "# Regression metrics\n",
        "def regression_metrics(preds, targets):\n",
        "    \"\"\"\n",
        "    preds, targets: 1D tensors with all edges concatenated\n",
        "\n",
        "    Returns:\n",
        "      dict: MSE, MAE, RMSE\n",
        "    \"\"\"\n",
        "    diff = preds - targets\n",
        "    mse = torch.mean(diff ** 2).item()\n",
        "    mae = torch.mean(diff.abs()).item()\n",
        "    rmse = mse ** 0.5\n",
        "    return {\"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "feiGPhWCxcuy"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_y_stats_from_loader(loader, device):\n",
        "    \"\"\"\n",
        "    Compute mean and std of the raw delay y from the training loader.\n",
        "    \"\"\"\n",
        "    ys = []\n",
        "    for _, _, _, y in loader:\n",
        "        ys.append(y.view(-1).float())\n",
        "    all_y = torch.cat(ys, dim=0).to(device)\n",
        "\n",
        "    y_mean = all_y.mean()\n",
        "    y_std = all_y.std()\n",
        "\n",
        "    # avoid division by zero\n",
        "    if y_std.item() < 1e-6:\n",
        "        y_std = y_std.new_tensor(1.0)\n",
        "\n",
        "    return y_mean, y_std\n",
        "\n",
        "def train_one_epoch_norm(model, loader, optimizer, device, y_mean, y_std):\n",
        "    \"\"\"\n",
        "    Train for one epoch using normalized targets:\n",
        "      y_norm = (y - y_mean) / y_std\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_edges = 0\n",
        "\n",
        "    for node_seq, edge_index, edge_attr, y in loader:\n",
        "        # node_seq:  (B=1, T, N, F_node)\n",
        "        # edge_index:(B=1, 2, E)\n",
        "        # edge_attr: (B=1, E, F_edge)\n",
        "        # y:         (B=1, E)\n",
        "        node_seq = node_seq.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "        edge_attr = edge_attr.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # normalize targets\n",
        "        y_norm = (y - y_mean) / y_std\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds_norm = model(node_seq, edge_index, edge_attr)  # (B=1, E), in normalized space\n",
        "        preds_norm = preds_norm.view_as(y_norm)\n",
        "\n",
        "        loss = loss_fn(preds_norm, y_norm)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_edges = y.numel()\n",
        "        total_loss += loss.item() * num_edges\n",
        "        total_edges += num_edges\n",
        "\n",
        "    # average MSE in normalized space\n",
        "    return total_loss / max(total_edges, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_denorm(model, loader, device, y_mean, y_std):\n",
        "    \"\"\"\n",
        "    Evaluate the model, denormalizing predictions back to minutes before\n",
        "    computing metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for node_seq, edge_index, edge_attr, y in loader:\n",
        "        node_seq = node_seq.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "        edge_attr = edge_attr.to(device)\n",
        "        y = y.to(device)  # raw minutes\n",
        "\n",
        "        preds_norm = model(node_seq, edge_index, edge_attr)  # (B=1, E), normalized\n",
        "        preds_norm = preds_norm.view(-1)\n",
        "\n",
        "        # denormalize predictions back to minutes\n",
        "        preds = preds_norm * y_std + y_mean\n",
        "        targets = y.view(-1)  # already in minutes\n",
        "\n",
        "        all_preds.append(preds.detach().cpu())\n",
        "        all_targets.append(targets.detach().cpu())\n",
        "\n",
        "    if not all_preds:\n",
        "        return {\"MSE\": float(\"nan\"), \"MAE\": float(\"nan\"), \"RMSE\": float(\"nan\")}\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "    return regression_metrics(all_preds, all_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKc4i95Fpt7q",
        "outputId": "84a673ac-4404-4d73-f765-46f0ed978526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Months present in dataset (YYYY-MM):\n",
            "['2013-01', '2013-02', '2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10', '2013-11', '2013-12']\n",
            "Experiment 1 - #train sequences: 1089, #test sequences: 368\n",
            "Experiment 2 - #train sequences: 1005, #test sequences: 452\n",
            "Adjacency (Exp1) shape: torch.Size([107, 107])\n",
            "Adjacency (Exp2) shape: torch.Size([107, 107])\n",
            "DataLoaders for both experiments are ready.\n"
          ]
        }
      ],
      "source": [
        "# Different training strategies - Comparing prediction results\n",
        "# (1) Trained on first 9 months and tested on last 3 months\n",
        "# (2) Trained on first 3 weeks of each month and tested on last week of each month\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "DATASET_START = pd.Timestamp(\"2013-01-01 00:00:00\")\n",
        "BLOCK_HOURS = 6  # 6-hour blocks\n",
        "\n",
        "def block_to_timestamp(block_idx: int) -> pd.Timestamp:\n",
        "    return DATASET_START + pd.Timedelta(hours=BLOCK_HOURS * int(block_idx))\n",
        "\n",
        "def block_to_calendar_info(block_idx: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      year, month, week_of_month (0..3), month_index\n",
        "\n",
        "    week_of_month definition:\n",
        "      week 0: days  1- 7\n",
        "      week 1: days  8-14\n",
        "      week 2: days 15-21\n",
        "      week 3: days 22-end (the \"last week\" of the month)\n",
        "    \"\"\"\n",
        "    ts = block_to_timestamp(block_idx)\n",
        "    year = ts.year\n",
        "    month = ts.month\n",
        "\n",
        "    week_idx_raw = (ts.day - 1) // 7\n",
        "    week_of_month = min(week_idx_raw, 3)  # clamp so \"last week\" is always 3\n",
        "\n",
        "    month_index = year * 12 + (month - 1)\n",
        "    return year, month, week_of_month, month_index\n",
        "\n",
        "# Build metadata for each sequence sample in seq_dataset\n",
        "\n",
        "sample_meta = []\n",
        "\n",
        "for ds_idx in range(len(seq_dataset)):\n",
        "    # seq_dataset maps indices -> positions in the underlying PyG dataset\n",
        "    pos = seq_dataset.valid_positions[ds_idx]\n",
        "    graph_idx = seq_dataset.sorted_indices[pos]\n",
        "    g = dataset[graph_idx]\n",
        "\n",
        "    if not hasattr(g, \"block_idx\"):\n",
        "        raise AttributeError(\n",
        "            \"Data objects must have a `block_idx` attribute to build calendar-based splits.\"\n",
        "        )\n",
        "\n",
        "    block_idx = int(g.block_idx)\n",
        "    year, month, week_of_month, month_index = block_to_calendar_info(block_idx)\n",
        "\n",
        "    sample_meta.append(\n",
        "        {\n",
        "            \"ds_idx\": ds_idx,          # index in seq_dataset\n",
        "            \"graph_idx\": graph_idx,    # index in original dataset\n",
        "            \"block_idx\": block_idx,\n",
        "            \"year\": year,\n",
        "            \"month\": month,\n",
        "            \"week_of_month\": week_of_month,\n",
        "            \"month_index\": month_index,\n",
        "        }\n",
        "    )\n",
        "\n",
        "unique_months = sorted({m[\"month_index\"] for m in sample_meta})\n",
        "\n",
        "def month_index_to_str(midx: int) -> str:\n",
        "    y = midx // 12\n",
        "    m = midx % 12 + 1\n",
        "    return f\"{y}-{m:02d}\"\n",
        "\n",
        "print(\"Months present in dataset (YYYY-MM):\")\n",
        "print([month_index_to_str(m) for m in unique_months])\n",
        "\n",
        "# Strategy (1): first 9 months vs last 3 months\n",
        "\n",
        "n_months = len(unique_months)\n",
        "if n_months < 12:\n",
        "    print(\n",
        "        f\"[WARN] Only {n_months} unique months found; \"\n",
        "        \"experiment (1) assumes at least 12. \"\n",
        "        \"Falling back to first 75% of months for train, last 25% for test.\"\n",
        "    )\n",
        "\n",
        "if n_months >= 12:\n",
        "    # earliest 9 months for train, latest 3 for test\n",
        "    train_months_exp1 = set(unique_months[:9])\n",
        "    test_months_exp1  = set(unique_months[-3:])\n",
        "else:\n",
        "    split = max(1, int(0.75 * n_months))\n",
        "    train_months_exp1 = set(unique_months[:split])\n",
        "    test_months_exp1  = set(unique_months[split:])\n",
        "\n",
        "exp1_train_indices = [m[\"ds_idx\"] for m in sample_meta if m[\"month_index\"] in train_months_exp1]\n",
        "exp1_test_indices  = [m[\"ds_idx\"] for m in sample_meta if m[\"month_index\"] in test_months_exp1]\n",
        "\n",
        "print(f\"Experiment 1 - #train sequences: {len(exp1_train_indices)}, #test sequences: {len(exp1_test_indices)}\")\n",
        "\n",
        "# Strategy (2): first 3 weeks vs last week of each month\n",
        "\n",
        "exp2_train_indices = [m[\"ds_idx\"] for m in sample_meta if m[\"week_of_month\"] < 3]\n",
        "exp2_test_indices  = [m[\"ds_idx\"] for m in sample_meta if m[\"week_of_month\"] == 3]\n",
        "\n",
        "print(f\"Experiment 2 - #train sequences: {len(exp2_train_indices)}, #test sequences: {len(exp2_test_indices)}\")\n",
        "\n",
        "adj_exp1 = build_adj_from_seq_subset(seq_dataset, exp1_train_indices)\n",
        "adj_exp2 = build_adj_from_seq_subset(seq_dataset, exp2_train_indices)\n",
        "\n",
        "print(\"Adjacency (Exp1) shape:\", adj_exp1.shape)\n",
        "print(\"Adjacency (Exp2) shape:\", adj_exp2.shape)\n",
        "\n",
        "# DataLoaders for both strategies\n",
        "\n",
        "batch_size = 1  # same as your original T-GCN training\n",
        "\n",
        "exp1_train_loader = DataLoader(Subset(seq_dataset, exp1_train_indices),\n",
        "                               batch_size=batch_size, shuffle=True)\n",
        "exp1_test_loader  = DataLoader(Subset(seq_dataset, exp1_test_indices),\n",
        "                               batch_size=batch_size, shuffle=False)\n",
        "\n",
        "exp2_train_loader = DataLoader(Subset(seq_dataset, exp2_train_indices),\n",
        "                               batch_size=batch_size, shuffle=True)\n",
        "exp2_test_loader  = DataLoader(Subset(seq_dataset, exp2_test_indices),\n",
        "                               batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders for both experiments are ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctJVTCPzCsha",
        "outputId": "52b5bf8e-eb8d-49af-c4a7-050628693ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Exp1: first 9 months vs last 3 months\n",
            "Target normalization: mean=13.384, std=41.558\n",
            "Epoch 001 | Train (norm MSE): 1.2785 | Test MSE: 2996.770, MAE: 37.626, RMSE: 54.743\n",
            "Epoch 002 | Train (norm MSE): 1.2333 | Test MSE: 1658.841, MAE: 33.491, RMSE: 40.729\n",
            "Epoch 003 | Train (norm MSE): 1.2459 | Test MSE: 1107.792, MAE: 19.139, RMSE: 33.284\n",
            "Epoch 004 | Train (norm MSE): 1.1525 | Test MSE: 1153.684, MAE: 16.790, RMSE: 33.966\n",
            "Epoch 005 | Train (norm MSE): 1.1356 | Test MSE: 2523.551, MAE: 41.367, RMSE: 50.235\n",
            "Epoch 006 | Train (norm MSE): 1.1473 | Test MSE: 1139.415, MAE: 16.216, RMSE: 33.755\n",
            "Epoch 007 | Train (norm MSE): 1.1119 | Test MSE: 1357.001, MAE: 27.617, RMSE: 36.837\n",
            "Epoch 008 | Train (norm MSE): 1.1129 | Test MSE: 2120.133, MAE: 37.882, RMSE: 46.045\n",
            "Epoch 009 | Train (norm MSE): 1.1229 | Test MSE: 1154.454, MAE: 22.071, RMSE: 33.977\n",
            "Epoch 010 | Train (norm MSE): 1.0861 | Test MSE: 1514.777, MAE: 22.129, RMSE: 38.920\n",
            "Epoch 011 | Train (norm MSE): 1.0753 | Test MSE: 1146.785, MAE: 21.567, RMSE: 33.864\n",
            "Epoch 012 | Train (norm MSE): 1.1011 | Test MSE: 1130.330, MAE: 21.671, RMSE: 33.620\n",
            "Epoch 013 | Train (norm MSE): 1.0716 | Test MSE: 1088.427, MAE: 17.852, RMSE: 32.991\n",
            "Epoch 014 | Train (norm MSE): 1.0464 | Test MSE: 1256.412, MAE: 18.362, RMSE: 35.446\n",
            "Epoch 015 | Train (norm MSE): 1.0507 | Test MSE: 1160.652, MAE: 21.733, RMSE: 34.068\n",
            "Epoch 016 | Train (norm MSE): 1.0307 | Test MSE: 1368.127, MAE: 19.893, RMSE: 36.988\n",
            "Epoch 017 | Train (norm MSE): 1.0499 | Test MSE: 1134.066, MAE: 19.885, RMSE: 33.676\n",
            "Epoch 018 | Train (norm MSE): 1.0168 | Test MSE: 1186.042, MAE: 22.944, RMSE: 34.439\n",
            "Epoch 019 | Train (norm MSE): 1.0253 | Test MSE: 1086.930, MAE: 18.364, RMSE: 32.969\n",
            "Epoch 020 | Train (norm MSE): 1.0207 | Test MSE: 1125.014, MAE: 20.495, RMSE: 33.541\n",
            "Epoch 021 | Train (norm MSE): 1.0034 | Test MSE: 1213.305, MAE: 23.671, RMSE: 34.833\n",
            "Epoch 022 | Train (norm MSE): 0.9977 | Test MSE: 1129.658, MAE: 20.821, RMSE: 33.610\n",
            "Epoch 023 | Train (norm MSE): 0.9976 | Test MSE: 1223.965, MAE: 23.807, RMSE: 34.985\n",
            "Epoch 024 | Train (norm MSE): 0.9926 | Test MSE: 1135.154, MAE: 15.967, RMSE: 33.692\n",
            "Epoch 025 | Train (norm MSE): 0.9903 | Test MSE: 1082.656, MAE: 17.595, RMSE: 32.904\n",
            "Epoch 026 | Train (norm MSE): 0.9959 | Test MSE: 1087.647, MAE: 18.558, RMSE: 32.979\n",
            "Epoch 027 | Train (norm MSE): 0.9778 | Test MSE: 1197.807, MAE: 23.377, RMSE: 34.609\n",
            "Epoch 028 | Train (norm MSE): 0.9777 | Test MSE: 1092.252, MAE: 18.005, RMSE: 33.049\n",
            "Epoch 029 | Train (norm MSE): 0.9831 | Test MSE: 1193.348, MAE: 16.932, RMSE: 34.545\n",
            "Epoch 030 | Train (norm MSE): 0.9883 | Test MSE: 1091.641, MAE: 16.556, RMSE: 33.040\n",
            "Epoch 031 | Train (norm MSE): 0.9815 | Test MSE: 1113.143, MAE: 20.243, RMSE: 33.364\n",
            "Epoch 032 | Train (norm MSE): 0.9769 | Test MSE: 1081.782, MAE: 17.600, RMSE: 32.890\n",
            "Epoch 033 | Train (norm MSE): 0.9733 | Test MSE: 1081.921, MAE: 17.831, RMSE: 32.893\n",
            "Epoch 034 | Train (norm MSE): 0.9766 | Test MSE: 1106.579, MAE: 20.045, RMSE: 33.265\n",
            "Epoch 035 | Train (norm MSE): 0.9740 | Test MSE: 1086.557, MAE: 17.624, RMSE: 32.963\n",
            "Epoch 036 | Train (norm MSE): 0.9730 | Test MSE: 1083.209, MAE: 18.455, RMSE: 32.912\n",
            "Epoch 037 | Train (norm MSE): 0.9707 | Test MSE: 1083.908, MAE: 17.977, RMSE: 32.923\n",
            "Epoch 038 | Train (norm MSE): 0.9729 | Test MSE: 1109.911, MAE: 16.038, RMSE: 33.315\n",
            "Epoch 039 | Train (norm MSE): 0.9678 | Test MSE: 1092.947, MAE: 16.447, RMSE: 33.060\n",
            "Epoch 040 | Train (norm MSE): 0.9729 | Test MSE: 1086.648, MAE: 18.389, RMSE: 32.964\n",
            "Epoch 041 | Train (norm MSE): 0.9684 | Test MSE: 1087.634, MAE: 17.953, RMSE: 32.979\n",
            "Epoch 042 | Train (norm MSE): 0.9730 | Test MSE: 1086.991, MAE: 17.994, RMSE: 32.970\n",
            "Epoch 043 | Train (norm MSE): 0.9659 | Test MSE: 1163.629, MAE: 22.344, RMSE: 34.112\n",
            "Epoch 044 | Train (norm MSE): 0.9671 | Test MSE: 1096.051, MAE: 16.546, RMSE: 33.107\n",
            "Epoch 045 | Train (norm MSE): 0.9727 | Test MSE: 1099.753, MAE: 19.654, RMSE: 33.163\n",
            "Epoch 046 | Train (norm MSE): 0.9715 | Test MSE: 1086.370, MAE: 17.165, RMSE: 32.960\n",
            "Epoch 047 | Train (norm MSE): 0.9700 | Test MSE: 1084.831, MAE: 17.817, RMSE: 32.937\n",
            "Epoch 048 | Train (norm MSE): 0.9693 | Test MSE: 1082.223, MAE: 18.405, RMSE: 32.897\n",
            "Epoch 049 | Train (norm MSE): 0.9670 | Test MSE: 1080.933, MAE: 18.157, RMSE: 32.878\n",
            "Epoch 050 | Train (norm MSE): 0.9700 | Test MSE: 1095.034, MAE: 19.135, RMSE: 33.091\n",
            "Epoch 051 | Train (norm MSE): 0.9641 | Test MSE: 1083.215, MAE: 17.471, RMSE: 32.912\n",
            "Epoch 052 | Train (norm MSE): 0.9678 | Test MSE: 1088.475, MAE: 16.264, RMSE: 32.992\n",
            "Epoch 053 | Train (norm MSE): 0.9663 | Test MSE: 1077.516, MAE: 17.231, RMSE: 32.826\n",
            "Epoch 054 | Train (norm MSE): 0.9618 | Test MSE: 1098.688, MAE: 20.064, RMSE: 33.146\n",
            "Epoch 055 | Train (norm MSE): 0.9601 | Test MSE: 1087.814, MAE: 16.583, RMSE: 32.982\n",
            "Epoch 056 | Train (norm MSE): 0.9672 | Test MSE: 1080.167, MAE: 16.911, RMSE: 32.866\n",
            "Epoch 057 | Train (norm MSE): 0.9621 | Test MSE: 1093.604, MAE: 16.625, RMSE: 33.070\n",
            "Epoch 058 | Train (norm MSE): 0.9648 | Test MSE: 1125.508, MAE: 20.841, RMSE: 33.549\n",
            "Epoch 059 | Train (norm MSE): 0.9616 | Test MSE: 1077.714, MAE: 18.030, RMSE: 32.829\n",
            "Epoch 060 | Train (norm MSE): 0.9542 | Test MSE: 1084.367, MAE: 17.132, RMSE: 32.930\n",
            "Epoch 061 | Train (norm MSE): 0.9588 | Test MSE: 1072.305, MAE: 17.514, RMSE: 32.746\n",
            "Epoch 062 | Train (norm MSE): 0.9537 | Test MSE: 1086.244, MAE: 16.939, RMSE: 32.958\n",
            "Epoch 063 | Train (norm MSE): 0.9559 | Test MSE: 1103.554, MAE: 16.299, RMSE: 33.220\n",
            "Epoch 064 | Train (norm MSE): 0.9545 | Test MSE: 1093.406, MAE: 16.094, RMSE: 33.067\n",
            "Epoch 065 | Train (norm MSE): 0.9600 | Test MSE: 1079.117, MAE: 16.980, RMSE: 32.850\n",
            "Epoch 066 | Train (norm MSE): 0.9521 | Test MSE: 1064.833, MAE: 17.778, RMSE: 32.632\n",
            "Epoch 067 | Train (norm MSE): 0.9542 | Test MSE: 1102.777, MAE: 15.618, RMSE: 33.208\n",
            "Epoch 068 | Train (norm MSE): 0.9510 | Test MSE: 1080.603, MAE: 16.887, RMSE: 32.873\n",
            "Epoch 069 | Train (norm MSE): 0.9471 | Test MSE: 1142.592, MAE: 21.593, RMSE: 33.802\n",
            "Epoch 070 | Train (norm MSE): 0.9519 | Test MSE: 1078.089, MAE: 17.087, RMSE: 32.834\n",
            "Epoch 071 | Train (norm MSE): 0.9503 | Test MSE: 1071.577, MAE: 18.099, RMSE: 32.735\n",
            "Epoch 072 | Train (norm MSE): 0.9492 | Test MSE: 1080.833, MAE: 17.175, RMSE: 32.876\n",
            "Epoch 073 | Train (norm MSE): 0.9482 | Test MSE: 1082.891, MAE: 15.948, RMSE: 32.907\n",
            "Epoch 074 | Train (norm MSE): 0.9511 | Test MSE: 1065.559, MAE: 17.386, RMSE: 32.643\n",
            "Epoch 075 | Train (norm MSE): 0.9434 | Test MSE: 1069.871, MAE: 16.721, RMSE: 32.709\n",
            "Epoch 076 | Train (norm MSE): 0.9457 | Test MSE: 1073.447, MAE: 17.372, RMSE: 32.763\n",
            "Epoch 077 | Train (norm MSE): 0.9490 | Test MSE: 1070.441, MAE: 17.734, RMSE: 32.718\n",
            "Epoch 078 | Train (norm MSE): 0.9479 | Test MSE: 1087.639, MAE: 16.268, RMSE: 32.979\n",
            "Epoch 079 | Train (norm MSE): 0.9449 | Test MSE: 1124.560, MAE: 15.469, RMSE: 33.534\n",
            "Epoch 080 | Train (norm MSE): 0.9427 | Test MSE: 1067.016, MAE: 17.935, RMSE: 32.665\n",
            "Epoch 081 | Train (norm MSE): 0.9380 | Test MSE: 1085.337, MAE: 15.812, RMSE: 32.944\n",
            "Epoch 082 | Train (norm MSE): 0.9469 | Test MSE: 1066.333, MAE: 17.633, RMSE: 32.655\n",
            "Epoch 083 | Train (norm MSE): 0.9476 | Test MSE: 1083.108, MAE: 16.102, RMSE: 32.911\n",
            "Epoch 084 | Train (norm MSE): 0.9425 | Test MSE: 1094.635, MAE: 15.877, RMSE: 33.085\n",
            "Epoch 085 | Train (norm MSE): 0.9485 | Test MSE: 1076.233, MAE: 15.983, RMSE: 32.806\n",
            "Epoch 086 | Train (norm MSE): 0.9430 | Test MSE: 1082.706, MAE: 16.230, RMSE: 32.904\n",
            "Epoch 087 | Train (norm MSE): 0.9527 | Test MSE: 1112.268, MAE: 15.460, RMSE: 33.351\n",
            "Epoch 088 | Train (norm MSE): 0.9443 | Test MSE: 1079.569, MAE: 16.692, RMSE: 32.857\n",
            "Epoch 089 | Train (norm MSE): 0.9417 | Test MSE: 1068.295, MAE: 18.203, RMSE: 32.685\n",
            "Epoch 090 | Train (norm MSE): 0.9428 | Test MSE: 1069.248, MAE: 16.655, RMSE: 32.699\n",
            "Epoch 091 | Train (norm MSE): 0.9403 | Test MSE: 1077.319, MAE: 16.067, RMSE: 32.823\n",
            "Epoch 092 | Train (norm MSE): 0.9412 | Test MSE: 1078.056, MAE: 19.219, RMSE: 32.834\n",
            "Epoch 093 | Train (norm MSE): 0.9356 | Test MSE: 1069.305, MAE: 17.169, RMSE: 32.700\n",
            "Epoch 094 | Train (norm MSE): 0.9399 | Test MSE: 1068.243, MAE: 17.892, RMSE: 32.684\n",
            "Epoch 095 | Train (norm MSE): 0.9448 | Test MSE: 1106.521, MAE: 19.827, RMSE: 33.264\n",
            "Epoch 096 | Train (norm MSE): 0.9474 | Test MSE: 1087.044, MAE: 16.609, RMSE: 32.970\n",
            "Epoch 097 | Train (norm MSE): 0.9387 | Test MSE: 1069.148, MAE: 18.122, RMSE: 32.698\n",
            "Epoch 098 | Train (norm MSE): 0.9398 | Test MSE: 1076.940, MAE: 16.705, RMSE: 32.817\n",
            "Epoch 099 | Train (norm MSE): 0.9414 | Test MSE: 1070.568, MAE: 16.465, RMSE: 32.720\n",
            "Epoch 100 | Train (norm MSE): 0.9432 | Test MSE: 1077.063, MAE: 16.407, RMSE: 32.819\n",
            "\n",
            "Best Exp1: first 9 months vs last 3 months epoch: 087 | Train (norm MSE): 0.9527 | Test MSE: 1112.268, MAE: 15.460, RMSE: 33.351\n",
            "\n",
            "Exp2: first 3 weeks vs last week of each month\n",
            "Target normalization: mean=11.997, std=39.348\n",
            "Epoch 001 | Train (norm MSE): 1.5733 | Test MSE: 1637.074, MAE: 20.727, RMSE: 40.461\n",
            "Epoch 002 | Train (norm MSE): 1.2530 | Test MSE: 1633.352, MAE: 23.305, RMSE: 40.415\n",
            "Epoch 003 | Train (norm MSE): 1.3383 | Test MSE: 1831.064, MAE: 21.470, RMSE: 42.791\n",
            "Epoch 004 | Train (norm MSE): 1.1819 | Test MSE: 1607.939, MAE: 21.805, RMSE: 40.099\n",
            "Epoch 005 | Train (norm MSE): 1.2772 | Test MSE: 1610.541, MAE: 20.323, RMSE: 40.132\n",
            "Epoch 006 | Train (norm MSE): 1.2087 | Test MSE: 2051.544, MAE: 33.420, RMSE: 45.294\n",
            "Epoch 007 | Train (norm MSE): 1.1456 | Test MSE: 2711.583, MAE: 40.516, RMSE: 52.073\n",
            "Epoch 008 | Train (norm MSE): 1.1593 | Test MSE: 1593.807, MAE: 22.158, RMSE: 39.923\n",
            "Epoch 009 | Train (norm MSE): 1.1433 | Test MSE: 1753.489, MAE: 19.450, RMSE: 41.875\n",
            "Epoch 010 | Train (norm MSE): 1.1578 | Test MSE: 1602.393, MAE: 25.604, RMSE: 40.030\n",
            "Epoch 011 | Train (norm MSE): 1.0961 | Test MSE: 1769.200, MAE: 27.151, RMSE: 42.062\n",
            "Epoch 012 | Train (norm MSE): 1.1498 | Test MSE: 1586.218, MAE: 22.012, RMSE: 39.827\n",
            "Epoch 013 | Train (norm MSE): 1.0767 | Test MSE: 2315.885, MAE: 36.293, RMSE: 48.124\n",
            "Epoch 014 | Train (norm MSE): 1.1103 | Test MSE: 2841.131, MAE: 33.157, RMSE: 53.302\n",
            "Epoch 015 | Train (norm MSE): 1.0895 | Test MSE: 1594.548, MAE: 20.265, RMSE: 39.932\n",
            "Epoch 016 | Train (norm MSE): 1.0629 | Test MSE: 1622.305, MAE: 24.833, RMSE: 40.278\n",
            "Epoch 017 | Train (norm MSE): 1.0459 | Test MSE: 1826.818, MAE: 30.983, RMSE: 42.741\n",
            "Epoch 018 | Train (norm MSE): 1.0180 | Test MSE: 1759.172, MAE: 20.818, RMSE: 41.942\n",
            "Epoch 019 | Train (norm MSE): 1.0140 | Test MSE: 1598.321, MAE: 21.341, RMSE: 39.979\n",
            "Epoch 020 | Train (norm MSE): 1.0166 | Test MSE: 1606.780, MAE: 22.634, RMSE: 40.085\n",
            "Epoch 021 | Train (norm MSE): 1.0167 | Test MSE: 1673.692, MAE: 26.321, RMSE: 40.911\n",
            "Epoch 022 | Train (norm MSE): 1.0040 | Test MSE: 1623.416, MAE: 23.144, RMSE: 40.292\n",
            "Epoch 023 | Train (norm MSE): 1.0053 | Test MSE: 1619.812, MAE: 20.444, RMSE: 40.247\n",
            "Epoch 024 | Train (norm MSE): 0.9951 | Test MSE: 1630.859, MAE: 19.847, RMSE: 40.384\n",
            "Epoch 025 | Train (norm MSE): 0.9977 | Test MSE: 1645.438, MAE: 19.191, RMSE: 40.564\n",
            "Epoch 026 | Train (norm MSE): 0.9901 | Test MSE: 1658.926, MAE: 19.170, RMSE: 40.730\n",
            "Epoch 027 | Train (norm MSE): 0.9890 | Test MSE: 1617.224, MAE: 20.226, RMSE: 40.215\n",
            "Epoch 028 | Train (norm MSE): 0.9830 | Test MSE: 1609.066, MAE: 20.296, RMSE: 40.113\n",
            "Epoch 029 | Train (norm MSE): 0.9843 | Test MSE: 1643.769, MAE: 19.333, RMSE: 40.543\n",
            "Epoch 030 | Train (norm MSE): 0.9812 | Test MSE: 1642.173, MAE: 19.463, RMSE: 40.524\n",
            "Epoch 031 | Train (norm MSE): 0.9864 | Test MSE: 1630.303, MAE: 19.776, RMSE: 40.377\n",
            "Epoch 032 | Train (norm MSE): 0.9847 | Test MSE: 1603.602, MAE: 22.479, RMSE: 40.045\n",
            "Epoch 033 | Train (norm MSE): 0.9821 | Test MSE: 1601.571, MAE: 20.453, RMSE: 40.020\n",
            "Epoch 034 | Train (norm MSE): 0.9826 | Test MSE: 1622.321, MAE: 19.774, RMSE: 40.278\n",
            "Epoch 035 | Train (norm MSE): 0.9738 | Test MSE: 1732.973, MAE: 19.049, RMSE: 41.629\n",
            "Epoch 036 | Train (norm MSE): 0.9793 | Test MSE: 1613.076, MAE: 19.731, RMSE: 40.163\n",
            "Epoch 037 | Train (norm MSE): 0.9787 | Test MSE: 1641.280, MAE: 19.353, RMSE: 40.513\n",
            "Epoch 038 | Train (norm MSE): 0.9722 | Test MSE: 1604.448, MAE: 19.888, RMSE: 40.056\n",
            "Epoch 039 | Train (norm MSE): 0.9735 | Test MSE: 1608.048, MAE: 19.618, RMSE: 40.100\n",
            "Epoch 040 | Train (norm MSE): 0.9700 | Test MSE: 1595.877, MAE: 20.924, RMSE: 39.948\n",
            "Epoch 041 | Train (norm MSE): 0.9699 | Test MSE: 1584.485, MAE: 22.672, RMSE: 39.806\n",
            "Epoch 042 | Train (norm MSE): 0.9781 | Test MSE: 1614.840, MAE: 20.029, RMSE: 40.185\n",
            "Epoch 043 | Train (norm MSE): 0.9727 | Test MSE: 1586.912, MAE: 20.169, RMSE: 39.836\n",
            "Epoch 044 | Train (norm MSE): 0.9701 | Test MSE: 1631.655, MAE: 19.165, RMSE: 40.394\n",
            "Epoch 045 | Train (norm MSE): 0.9689 | Test MSE: 1589.499, MAE: 20.739, RMSE: 39.869\n",
            "Epoch 046 | Train (norm MSE): 0.9686 | Test MSE: 1644.946, MAE: 19.225, RMSE: 40.558\n",
            "Epoch 047 | Train (norm MSE): 0.9725 | Test MSE: 1599.331, MAE: 20.093, RMSE: 39.992\n",
            "Epoch 048 | Train (norm MSE): 0.9737 | Test MSE: 1598.547, MAE: 19.814, RMSE: 39.982\n",
            "Epoch 049 | Train (norm MSE): 0.9717 | Test MSE: 1601.342, MAE: 19.875, RMSE: 40.017\n",
            "Epoch 050 | Train (norm MSE): 0.9654 | Test MSE: 1589.720, MAE: 21.295, RMSE: 39.871\n",
            "Epoch 051 | Train (norm MSE): 0.9641 | Test MSE: 1567.645, MAE: 21.144, RMSE: 39.593\n",
            "Epoch 052 | Train (norm MSE): 0.9669 | Test MSE: 1599.294, MAE: 19.883, RMSE: 39.991\n",
            "Epoch 053 | Train (norm MSE): 0.9636 | Test MSE: 1578.932, MAE: 22.088, RMSE: 39.736\n",
            "Epoch 054 | Train (norm MSE): 0.9650 | Test MSE: 1579.813, MAE: 20.807, RMSE: 39.747\n",
            "Epoch 055 | Train (norm MSE): 0.9654 | Test MSE: 1625.719, MAE: 19.054, RMSE: 40.320\n",
            "Epoch 056 | Train (norm MSE): 0.9601 | Test MSE: 1643.321, MAE: 18.959, RMSE: 40.538\n",
            "Epoch 057 | Train (norm MSE): 0.9622 | Test MSE: 1618.188, MAE: 19.965, RMSE: 40.227\n",
            "Epoch 058 | Train (norm MSE): 0.9609 | Test MSE: 1599.469, MAE: 19.755, RMSE: 39.993\n",
            "Epoch 059 | Train (norm MSE): 0.9580 | Test MSE: 1576.002, MAE: 22.060, RMSE: 39.699\n",
            "Epoch 060 | Train (norm MSE): 0.9631 | Test MSE: 1576.887, MAE: 21.170, RMSE: 39.710\n",
            "Epoch 061 | Train (norm MSE): 0.9625 | Test MSE: 1618.874, MAE: 19.482, RMSE: 40.235\n",
            "Epoch 062 | Train (norm MSE): 0.9642 | Test MSE: 1590.345, MAE: 21.238, RMSE: 39.879\n",
            "Epoch 063 | Train (norm MSE): 0.9523 | Test MSE: 1557.151, MAE: 20.998, RMSE: 39.461\n",
            "Epoch 064 | Train (norm MSE): 0.9646 | Test MSE: 1578.494, MAE: 21.372, RMSE: 39.730\n",
            "Epoch 065 | Train (norm MSE): 0.9645 | Test MSE: 1571.658, MAE: 21.159, RMSE: 39.644\n",
            "Epoch 066 | Train (norm MSE): 0.9621 | Test MSE: 1578.730, MAE: 20.526, RMSE: 39.733\n",
            "Epoch 067 | Train (norm MSE): 0.9517 | Test MSE: 1621.063, MAE: 19.491, RMSE: 40.262\n",
            "Epoch 068 | Train (norm MSE): 0.9600 | Test MSE: 1616.208, MAE: 19.981, RMSE: 40.202\n",
            "Epoch 069 | Train (norm MSE): 0.9602 | Test MSE: 1565.246, MAE: 20.723, RMSE: 39.563\n",
            "Epoch 070 | Train (norm MSE): 0.9515 | Test MSE: 1579.548, MAE: 20.419, RMSE: 39.744\n",
            "Epoch 071 | Train (norm MSE): 0.9582 | Test MSE: 1558.457, MAE: 20.770, RMSE: 39.477\n",
            "Epoch 072 | Train (norm MSE): 0.9560 | Test MSE: 1570.101, MAE: 19.653, RMSE: 39.624\n",
            "Epoch 073 | Train (norm MSE): 0.9558 | Test MSE: 1567.051, MAE: 20.317, RMSE: 39.586\n",
            "Epoch 074 | Train (norm MSE): 0.9551 | Test MSE: 1569.581, MAE: 19.562, RMSE: 39.618\n",
            "Epoch 075 | Train (norm MSE): 0.9567 | Test MSE: 1605.972, MAE: 19.623, RMSE: 40.075\n",
            "Epoch 076 | Train (norm MSE): 0.9473 | Test MSE: 1612.233, MAE: 19.781, RMSE: 40.153\n",
            "Epoch 077 | Train (norm MSE): 0.9528 | Test MSE: 1644.672, MAE: 18.880, RMSE: 40.555\n",
            "Epoch 078 | Train (norm MSE): 0.9520 | Test MSE: 1574.359, MAE: 19.591, RMSE: 39.678\n",
            "Epoch 079 | Train (norm MSE): 0.9492 | Test MSE: 1596.783, MAE: 19.486, RMSE: 39.960\n",
            "Epoch 080 | Train (norm MSE): 0.9574 | Test MSE: 1573.255, MAE: 19.715, RMSE: 39.664\n",
            "Epoch 081 | Train (norm MSE): 0.9521 | Test MSE: 1530.506, MAE: 21.303, RMSE: 39.122\n",
            "Epoch 082 | Train (norm MSE): 0.9502 | Test MSE: 1567.174, MAE: 19.551, RMSE: 39.588\n",
            "Epoch 083 | Train (norm MSE): 0.9491 | Test MSE: 1583.432, MAE: 20.366, RMSE: 39.792\n",
            "Epoch 084 | Train (norm MSE): 0.9486 | Test MSE: 1600.471, MAE: 19.348, RMSE: 40.006\n",
            "Epoch 085 | Train (norm MSE): 0.9509 | Test MSE: 1630.294, MAE: 19.651, RMSE: 40.377\n",
            "Epoch 086 | Train (norm MSE): 0.9607 | Test MSE: 1567.733, MAE: 19.524, RMSE: 39.595\n",
            "Epoch 087 | Train (norm MSE): 0.9500 | Test MSE: 1618.243, MAE: 19.083, RMSE: 40.227\n",
            "Epoch 088 | Train (norm MSE): 0.9515 | Test MSE: 1590.766, MAE: 19.317, RMSE: 39.884\n",
            "Epoch 089 | Train (norm MSE): 0.9527 | Test MSE: 1599.786, MAE: 19.297, RMSE: 39.997\n",
            "Epoch 090 | Train (norm MSE): 0.9481 | Test MSE: 1549.112, MAE: 20.683, RMSE: 39.359\n",
            "Epoch 091 | Train (norm MSE): 0.9469 | Test MSE: 1530.232, MAE: 20.587, RMSE: 39.118\n",
            "Epoch 092 | Train (norm MSE): 0.9510 | Test MSE: 1601.833, MAE: 19.522, RMSE: 40.023\n",
            "Epoch 093 | Train (norm MSE): 0.9507 | Test MSE: 1579.283, MAE: 19.544, RMSE: 39.740\n",
            "Epoch 094 | Train (norm MSE): 0.9481 | Test MSE: 1592.996, MAE: 19.451, RMSE: 39.912\n",
            "Epoch 095 | Train (norm MSE): 0.9482 | Test MSE: 1510.623, MAE: 21.721, RMSE: 38.867\n",
            "Epoch 096 | Train (norm MSE): 0.9420 | Test MSE: 1612.608, MAE: 18.722, RMSE: 40.157\n",
            "Epoch 097 | Train (norm MSE): 0.9490 | Test MSE: 1538.932, MAE: 20.073, RMSE: 39.229\n",
            "Epoch 098 | Train (norm MSE): 0.9452 | Test MSE: 1572.774, MAE: 19.779, RMSE: 39.658\n",
            "Epoch 099 | Train (norm MSE): 0.9450 | Test MSE: 1577.477, MAE: 19.766, RMSE: 39.717\n",
            "Epoch 100 | Train (norm MSE): 0.9517 | Test MSE: 1515.826, MAE: 21.515, RMSE: 38.934\n",
            "\n",
            "Best Exp2: first 3 weeks vs last week of each month epoch: 096 | Train (norm MSE): 0.9420 | Test MSE: 1612.608, MAE: 18.722, RMSE: 40.157\n",
            "\n",
            "Summary comparison (best MAE over epochs, in minutes)\n",
            "Exp1 best metrics: {'MSE': 1112.267822265625, 'MAE': 15.46037769317627, 'RMSE': 33.35067948731517}\n",
            "Exp2 best metrics: {'MSE': 1612.6082763671875, 'MAE': 18.721759796142578, 'RMSE': 40.157294186326695}\n"
          ]
        }
      ],
      "source": [
        "# Run the two T-GCN experiments (model selection based on MAE)\n",
        "\n",
        "import copy\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "def make_tgcn_model(adj_matrix):\n",
        "    \"\"\"Create a new T-GCN delay model with the given adjacency.\"\"\"\n",
        "    model = TGCNDelayModel(\n",
        "        adj=adj_matrix,\n",
        "        node_feat_dim=node_feat_dim,\n",
        "        edge_feat_dim=edge_feat_dim,\n",
        "        hidden_dim=128,\n",
        "        edge_hidden_dim=64,\n",
        "    ).to(device)\n",
        "    return model\n",
        "\n",
        "def run_experiment(train_loader, test_loader, adj_matrix, num_epochs=50, label=\"\"):\n",
        "    # 1) compute normalization stats only from training data\n",
        "    y_mean, y_std = compute_y_stats_from_loader(train_loader, device)\n",
        "    print(\n",
        "        f\"\\n{label}\\n\"\n",
        "        f\"Target normalization: mean={y_mean.item():.3f}, std={y_std.item():.3f}\"\n",
        "    )\n",
        "\n",
        "    model = make_tgcn_model(adj_matrix)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "    best_mae = float(\"inf\")\n",
        "    best_epoch = -1\n",
        "    best_metrics = None\n",
        "    best_train_loss = None  # normalized MSE\n",
        "    best_state_dict = None\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        # train on normalized y\n",
        "        train_loss = train_one_epoch_norm(model, train_loader, optimizer, device, y_mean, y_std)\n",
        "\n",
        "        # evaluate in original units (minutes)\n",
        "        test_metrics = evaluate_denorm(model, test_loader, device, y_mean, y_std)\n",
        "\n",
        "        # model selection based on MAE in minutes\n",
        "        is_best = test_metrics[\"MAE\"] < best_mae\n",
        "        if is_best:\n",
        "            best_mae = test_metrics[\"MAE\"]\n",
        "            best_epoch = epoch\n",
        "            best_metrics = test_metrics\n",
        "            best_train_loss = train_loss\n",
        "            best_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"Train (norm MSE): {train_loss:.4f} | \"\n",
        "            f\"Test MSE: {test_metrics['MSE']:.3f}, \"\n",
        "            f\"MAE: {test_metrics['MAE']:.3f}, \"\n",
        "            f\"RMSE: {test_metrics['RMSE']:.3f}\"\n",
        "        )\n",
        "\n",
        "    # Restore best model weights (based on MAE)\n",
        "    if best_state_dict is not None:\n",
        "        model.load_state_dict(best_state_dict)\n",
        "\n",
        "    # Print stats for the best epoch\n",
        "    print(\n",
        "        f\"\\nBest {label} epoch: {best_epoch:03d} | \"\n",
        "        f\"Train (norm MSE): {best_train_loss:.4f} | \"\n",
        "        f\"Test MSE: {best_metrics['MSE']:.3f}, \"\n",
        "        f\"MAE: {best_metrics['MAE']:.3f}, \"\n",
        "        f\"RMSE: {best_metrics['RMSE']:.3f}\"\n",
        "    )\n",
        "\n",
        "    # Return model and metrics in original units\n",
        "    return model, best_metrics\n",
        "\n",
        "# Number of epochs for the experiments\n",
        "exp_num_epochs = 100\n",
        "\n",
        "# (1) Trained on first 9 months and tested on last 3 months\n",
        "model_exp1, metrics_exp1 = run_experiment(\n",
        "    exp1_train_loader,\n",
        "    exp1_test_loader,\n",
        "    adj_exp1,\n",
        "    num_epochs=exp_num_epochs,\n",
        "    label=\"Exp1: first 9 months vs last 3 months\",\n",
        ")\n",
        "\n",
        "# (2) Trained on first 3 weeks of each month and tested on last week of each month\n",
        "model_exp2, metrics_exp2 = run_experiment(\n",
        "    exp2_train_loader,\n",
        "    exp2_test_loader,\n",
        "    adj_exp2,\n",
        "    num_epochs=exp_num_epochs,\n",
        "    label=\"Exp2: first 3 weeks vs last week of each month\",\n",
        ")\n",
        "\n",
        "print(\"\\nSummary comparison (best MAE over epochs, in minutes)\")\n",
        "print(\"Exp1 best metrics:\", metrics_exp1)\n",
        "print(\"Exp2 best metrics:\", metrics_exp2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
