{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Other experiments/models ran\n",
        "\n",
        "Other experiments that we ran based on how functional our baseline models were. None of these recieved great final MAE's so detailed explaination is not provided in the blog post."
      ],
      "metadata": {
        "id": "mHXSIwEINtBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "AKe_2qF0NPnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch geometric -- for pyg\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install -q torch-geometric"
      ],
      "metadata": {
        "id": "4Sd7KAiXN1up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "-XjHVGJXN2KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive"
      ],
      "metadata": {
        "id": "vNrU1YbYN3Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "GoaF8BgCSA5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset\n",
        "\n",
        "Replace dataset_dest with the folder where you have your skynet graphs"
      ],
      "metadata": {
        "id": "9sIUPL3lOABI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkyNetDataset(InMemoryDataset):\n",
        "  def __init__(self, root, transform=None, pre_transform=None):\n",
        "    super().__init__(root, transform, pre_transform)\n",
        "    self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return []\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return ['flight_graphs.pt']\n",
        "\n",
        "  def process(self):\n",
        "    pass\n",
        "\n",
        "dataset_dest = \"/content/drive/Shareddrives/CS_224W_Project/data/data/skynet_clean_graphs\"\n",
        "dataset = SkyNetDataset(root=dataset_dest)"
      ],
      "metadata": {
        "id": "KO63lQyqN4Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TGN"
      ],
      "metadata": {
        "id": "0uza8GMkP4aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formulate dataset for TGN"
      ],
      "metadata": {
        "id": "wv3zY59ZPmrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence dataset for TGN\n",
        "# Same as dataset for TGCN\n",
        "class SequenceSkyNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps your PyG SkyNetDataset into sequences for T-GCN.\n",
        "\n",
        "    Each item:\n",
        "      node_seq:   (T, N, F_node)   – node features for T consecutive time steps\n",
        "      edge_index: (2, E)           – edges of the last time step\n",
        "      edge_attr:  (E, F_edge)      – edge features of the last time step\n",
        "      y:          (E,)             – departure delay labels of the last time step\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset, history_len=4, require_edges=True, binary_threshold=15):\n",
        "        self.base = base_dataset\n",
        "        self.history_len = history_len\n",
        "        self.require_edges = require_edges\n",
        "        self.binary_threshold = binary_threshold\n",
        "\n",
        "        # Compute edge_attr normalization stats\n",
        "        all_edge_attrs = []\n",
        "        for i in range(len(base_dataset)):\n",
        "            if base_dataset[i].edge_attr.numel() > 0:\n",
        "                all_edge_attrs.append(base_dataset[i].edge_attr)\n",
        "\n",
        "        all_edge_attrs = torch.cat(all_edge_attrs, dim=0)\n",
        "        self.edge_mean = all_edge_attrs.mean(dim=0)\n",
        "        self.edge_std = all_edge_attrs.std(dim=0)\n",
        "        self.edge_std[self.edge_std < 1e-6] = 1.0\n",
        "\n",
        "\n",
        "        # Sort indices by time or block_idx if available\n",
        "        indices = list(range(len(base_dataset)))\n",
        "        if hasattr(base_dataset[0], \"block_idx\"):\n",
        "            indices = sorted(indices, key=lambda i: int(base_dataset[i].block_idx))\n",
        "\n",
        "        self.sorted_indices = indices\n",
        "\n",
        "        # Build list of valid target positions\n",
        "        valid_positions = []\n",
        "        for pos in range(history_len - 1, len(self.sorted_indices)):\n",
        "            idx = self.sorted_indices[pos]\n",
        "            data = self.base[idx]\n",
        "            if require_edges and data.edge_index.size(1) == 0:\n",
        "                continue\n",
        "            valid_positions.append(pos)\n",
        "\n",
        "        self.valid_positions = valid_positions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_positions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        idx indexes into valid_positions, not directly into base_dataset.\n",
        "        \"\"\"\n",
        "        pos = self.valid_positions[idx]\n",
        "        # history positions for this target\n",
        "        hist_positions = self.sorted_indices[pos - self.history_len + 1 : pos + 1]\n",
        "\n",
        "        graphs = [self.base[i] for i in hist_positions]\n",
        "\n",
        "        # Node sequence: (T, N, F_node)\n",
        "        node_seq = torch.stack([g.x for g in graphs], dim=0)\n",
        "\n",
        "        # Last graph provides edges, edge features, labels\n",
        "        target = graphs[-1]\n",
        "        edge_index = target.edge_index\n",
        "        edge_attr = (target.edge_attr - self.edge_mean) / self.edge_std\n",
        "        y = target.y\n",
        "\n",
        "        return node_seq, edge_index, edge_attr, y"
      ],
      "metadata": {
        "id": "KkB-xbWTOwYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build sequence dataset\n",
        "history_len = 8  # 8 * 6h = 48h of history\n",
        "seq_dataset = SequenceSkyNetDataset(dataset, history_len=history_len)\n",
        "\n",
        "num_samples = len(seq_dataset)\n",
        "train_end = int(0.7 * num_samples)\n",
        "val_end   = int(0.85 * num_samples)\n",
        "\n",
        "train_set = torch.utils.data.Subset(seq_dataset, range(0, train_end))\n",
        "val_set   = torch.utils.data.Subset(seq_dataset, range(train_end, val_end))\n",
        "test_set  = torch.utils.data.Subset(seq_dataset, range(val_end, num_samples))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
        "val_loader   = DataLoader(val_set,   batch_size=1, shuffle=False)\n",
        "test_loader  = DataLoader(test_set,  batch_size=1, shuffle=False)\n",
        "\n",
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ],
      "metadata": {
        "id": "cG5T7dYvOw1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset wrapper for TGN\n",
        "class TGNFlightDataset:\n",
        "    \"\"\"\n",
        "    Converts SkynetDataset to TGN format.\n",
        "\n",
        "    TGN expects:\n",
        "    - Chronologically ordered interactions (flights)\n",
        "    - Each interaction: (src, dst, time, edge_features)\n",
        "    \"\"\"\n",
        "    def __init__(self, subset_or_dataset):\n",
        "        # Unwrap if it's a Subset\n",
        "        if isinstance(subset_or_dataset, torch.utils.data.Subset):\n",
        "            self.seq_dataset = subset_or_dataset.dataset\n",
        "            self.indices = subset_or_dataset.indices\n",
        "        else:\n",
        "            self.seq_dataset = subset_or_dataset\n",
        "            self.indices = list(range(len(subset_or_dataset)))\n",
        "\n",
        "        self.interactions = []\n",
        "\n",
        "        # Extract all flights chronologically\n",
        "        print(\"Building TGN dataset...\")\n",
        "        for subset_idx in self.indices:\n",
        "            node_seq, edge_index, edge_attr, y = self.seq_dataset[subset_idx]\n",
        "\n",
        "            pos = self.seq_dataset.valid_positions[subset_idx]\n",
        "            graph_idx = self.seq_dataset.sorted_indices[pos]\n",
        "            graph = self.seq_dataset.base[graph_idx]\n",
        "\n",
        "            timestamp = int(graph.block_idx) if hasattr(graph, 'block_idx') else int(subset_idx)\n",
        "\n",
        "            # Get node features (use last time step)\n",
        "            node_features = node_seq[-1]\n",
        "\n",
        "            # Handle edge_index shape\n",
        "            if edge_index.dim() == 3:\n",
        "                edge_index = edge_index[0]\n",
        "            src = edge_index[0]\n",
        "            dst = edge_index[1]\n",
        "\n",
        "            # Handle y and edge_attr shapes\n",
        "            if y.dim() == 2:\n",
        "                y = y[0]\n",
        "            if edge_attr.dim() == 3:\n",
        "                edge_attr = edge_attr[0]\n",
        "\n",
        "            for e in range(len(src)):\n",
        "                self.interactions.append({\n",
        "                    'src': src[e].item(),\n",
        "                    'dst': dst[e].item(),\n",
        "                    'time': timestamp,\n",
        "                    'edge_attr': edge_attr[e],\n",
        "                    'label': y[e].item(),\n",
        "                    'node_features': node_features,\n",
        "                })\n",
        "\n",
        "    def get_temporal_batches(self, batch_size=200):\n",
        "        \"\"\"\n",
        "        Return batches of interactions in chronological order\n",
        "        \"\"\"\n",
        "        batches = []\n",
        "        for i in range(0, len(self.interactions), batch_size):\n",
        "            batch = self.interactions[i:i+batch_size]\n",
        "\n",
        "            src = torch.tensor([x['src'] for x in batch], dtype=torch.long)\n",
        "            dst = torch.tensor([x['dst'] for x in batch], dtype=torch.long)\n",
        "            t = torch.tensor([x['time'] for x in batch], dtype=torch.long)\n",
        "            edge_attr = torch.stack([x['edge_attr'] for x in batch])\n",
        "            labels = torch.tensor([x['label'] for x in batch], dtype=torch.float)\n",
        "            node_features = batch[0]['node_features']  # Assume same for batch\n",
        "\n",
        "            batches.append((src, dst, t, edge_attr, labels, node_features))\n",
        "\n",
        "        return batches"
      ],
      "metadata": {
        "id": "6rXVhMP5PvVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Training TGN ===\")\n",
        "tgn_train_dataset = TGNFlightDataset(train_set)\n",
        "tgn_test_dataset = TGNFlightDataset(test_set)"
      ],
      "metadata": {
        "id": "QfuC0BJXPzjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model formulation"
      ],
      "metadata": {
        "id": "NkRNkahoNUd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRmyohaoNNJW"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import TGNMemory\n",
        "from torch_geometric.nn.models.tgn import (\n",
        "    IdentityMessage,\n",
        "    LastAggregator,\n",
        ")\n",
        "\n",
        "# manual rewrite of TimeEncoder cause I cant figure out how to import it\n",
        "class TimeEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Time encoding module for TGN.\n",
        "    Encodes time as a learnable function.\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channels):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.lin = nn.Linear(1, out_channels)\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            t: [E] timestamps\n",
        "        Returns:\n",
        "            [E, out_channels] time encodings\n",
        "        \"\"\"\n",
        "        return self.lin(t.view(-1, 1))\n",
        "\n",
        "import copy\n",
        "\n",
        "class FlightTGN(nn.Module):\n",
        "    \"\"\"\n",
        "    Temporal Graph Network for flight delay prediction.\n",
        "\n",
        "    Args:\n",
        "        num_nodes: Number of airports\n",
        "        raw_msg_dim: Dimension of raw messages (node_dim + edge_dim + time_dim)\n",
        "        memory_dim: Dimension of memory embeddings\n",
        "        time_dim: Dimension of time encodings\n",
        "        embedding_dim: Final embedding dimension\n",
        "        edge_feat_dim: Dimension of edge features\n",
        "        node_feat_dim: Dimension of node features\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes, memory_dim=100, time_dim=100,\n",
        "                 embedding_dim=100, edge_feat_dim=8, node_feat_dim=9):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_nodes = num_nodes\n",
        "        self.memory_dim = memory_dim\n",
        "        self.time_dim = time_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Message dimension: node features + edge features + time encoding\n",
        "        raw_msg_dim = node_feat_dim + edge_feat_dim + time_dim\n",
        "\n",
        "        # TGN Memory module - tracks interaction history for each node\n",
        "        self.memory = TGNMemory(\n",
        "            num_nodes=num_nodes,\n",
        "            raw_msg_dim=edge_feat_dim + node_feat_dim + time_dim,\n",
        "            memory_dim=memory_dim,\n",
        "            time_dim=time_dim,\n",
        "            message_module=IdentityMessage(\n",
        "                raw_msg_dim=edge_feat_dim + node_feat_dim + time_dim,\n",
        "                memory_dim=memory_dim,\n",
        "                time_dim=time_dim\n",
        "            ),\n",
        "            aggregator_module=LastAggregator(),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.time_encoder = TimeEncoder(time_dim)\n",
        "\n",
        "        # Edge embedding module - combines src memory, dst memory, edge features\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(2 * memory_dim + edge_feat_dim + time_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "        )\n",
        "\n",
        "        # Prediction head - regression (delay in minutes)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dst, t, edge_attr, node_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Source node indices [E]\n",
        "            dst: Destination node indices [E]\n",
        "            t: Timestamps [E] (in seconds or block indices)\n",
        "            edge_attr: Edge features [E, edge_feat_dim]\n",
        "            node_features: Node features [N, node_feat_dim] (optional, can be None)\n",
        "\n",
        "        Returns:\n",
        "            predictions: [E] - predicted delay in minutes\n",
        "        \"\"\"\n",
        "        src_memory, _ = self.memory(src)  # [E, memory_dim]\n",
        "        dst_memory, _ = self.memory(dst)  # [E, memory_dim]\n",
        "\n",
        "        time_encoding = self.time_encoder(t.float())  # [E, time_dim]\n",
        "\n",
        "        if edge_attr.dim() == 1:\n",
        "            edge_attr = edge_attr.unsqueeze(0)\n",
        "\n",
        "        edge_emb = torch.cat([src_memory, dst_memory, edge_attr, time_encoding], dim=-1)\n",
        "        edge_emb = self.edge_encoder(edge_emb)\n",
        "\n",
        "        predictions = self.predictor(edge_emb).squeeze(-1)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def update_memory(self, src, dst, t, edge_attr, node_features):\n",
        "        \"\"\"Update memory after processing a batch of interactions\"\"\"\n",
        "        time_encoding = self.time_encoder(t.float())\n",
        "\n",
        "        if node_features is not None:\n",
        "            src_node_feat = node_features[src]\n",
        "            dst_node_feat = node_features[dst]\n",
        "        else:\n",
        "            src_node_feat = torch.zeros(len(src), 1, device=src.device)\n",
        "            dst_node_feat = torch.zeros(len(dst), 1, device=dst.device)\n",
        "\n",
        "        raw_msg = torch.cat([src_node_feat, edge_attr, time_encoding], dim=-1)\n",
        "        self.memory.update_state(src, dst, t, raw_msg)\n",
        "\n",
        "    def reset_memory(self):\n",
        "        \"\"\"Reset memory between epochs or train/test\"\"\"\n",
        "        self.memory.reset_state()\n",
        "\n",
        "\n",
        "\n",
        "def train_tgn(model, train_dataset, optimizer, device, normalize=True):\n",
        "    \"\"\"Train TGN for one epoch\"\"\"\n",
        "    model.train()\n",
        "    model.reset_memory()\n",
        "\n",
        "    batches = train_dataset.get_temporal_batches(batch_size=200)\n",
        "\n",
        "    if normalize:\n",
        "        all_labels = torch.cat([b[4] for b in batches])\n",
        "        y_mean = all_labels.mean()\n",
        "        y_std = all_labels.std()\n",
        "        y_std = y_std if y_std > 1e-6 else torch.tensor(1.0)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for src, dst, t, edge_attr, labels, node_features in batches:\n",
        "        src = src.to(device)\n",
        "        dst = dst.to(device)\n",
        "        t = t.to(device)\n",
        "        edge_attr = edge_attr.to(device)\n",
        "        labels = labels.to(device)\n",
        "        node_features = node_features.to(device)\n",
        "\n",
        "        if normalize:\n",
        "            labels_norm = (labels - y_mean.to(device)) / y_std.to(device)\n",
        "        else:\n",
        "            labels_norm = labels\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(src, dst, t, edge_attr, node_features)\n",
        "\n",
        "        loss = nn.MSELoss()(preds, labels_norm)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.update_memory(src, dst, t, edge_attr, node_features)\n",
        "\n",
        "        total_loss += loss.item() * len(labels)\n",
        "        total_samples += len(labels)\n",
        "\n",
        "    return total_loss / total_samples, y_mean if normalize else None, y_std if normalize else None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_tgn(model, test_dataset, device, y_mean=None, y_std=None):\n",
        "    \"\"\"Evaluate TGN\"\"\"\n",
        "    model.eval()\n",
        "    model.reset_memory()\n",
        "\n",
        "    batches = test_dataset.get_temporal_batches(batch_size=200)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for src, dst, t, edge_attr, labels, node_features in batches:\n",
        "        src = src.to(device)\n",
        "        dst = dst.to(device)\n",
        "        t = t.to(device)\n",
        "        edge_attr = edge_attr.to(device)\n",
        "        labels = labels.to(device)\n",
        "        node_features = node_features.to(device)\n",
        "\n",
        "        preds = model(src, dst, t, edge_attr, node_features)\n",
        "\n",
        "        if y_mean is not None and y_std is not None:\n",
        "            preds = preds * y_std.to(device) + y_mean.to(device)\n",
        "\n",
        "        model.update_memory(src, dst, t, edge_attr, node_features)\n",
        "\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    mse = ((all_preds - all_labels) ** 2).mean().item()\n",
        "    mae = (all_preds - all_labels).abs().mean().item()\n",
        "    rmse = mse ** 0.5\n",
        "\n",
        "    return {\"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "num_nodes = dataset[0].num_nodes\n",
        "model = FlightTGN(\n",
        "    num_nodes=num_nodes,\n",
        "    memory_dim=100,\n",
        "    time_dim=100,\n",
        "    embedding_dim=100,\n",
        "    edge_feat_dim=8,\n",
        "    node_feat_dim=9\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "2bxtqHjKNc4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "VCBrcOOHNdMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "best_mae = float('inf')\n",
        "best_metrics = None\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    train_loss, y_mean, y_std = train_tgn(model, tgn_train_dataset, optimizer, device)\n",
        "    test_metrics = evaluate_tgn(model, tgn_test_dataset, device, y_mean, y_std)\n",
        "\n",
        "    if test_metrics['MAE'] < best_mae:\n",
        "        best_mae = test_metrics['MAE']\n",
        "        best_metrics = test_metrics\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Test MAE: {test_metrics['MAE']:.2f}, RMSE: {test_metrics['RMSE']:.2f}\")\n",
        "\n",
        "print(f\"\\nBest Test Metrics: {best_metrics}\")"
      ],
      "metadata": {
        "id": "YgYrHuAmNe19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCN + temporal features"
      ],
      "metadata": {
        "id": "lp3c2ZaUP6_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "KanHkBUQQlLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkyNetDataset(InMemoryDataset):\n",
        "    def __init__(self, root, mode='month_split', transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.mode = mode\n",
        "\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
        "        data_list = [self.get(i) for i in range(len(self))]\n",
        "\n",
        "        new_list = []\n",
        "        for data in data_list:\n",
        "\n",
        "            # extract month/day from Python datetime\n",
        "            t = data.time\n",
        "            month = t.month\n",
        "            day = t.day\n",
        "\n",
        "            num_edges = data.edge_index.size(1)\n",
        "\n",
        "            # MONTH-SPLIT (whole graph train/test)\n",
        "            if self.mode == 'month_split':\n",
        "                if 1 <= month <= 9:\n",
        "                    train_mask = torch.ones(num_edges, dtype=torch.bool)\n",
        "                    test_mask  = torch.zeros(num_edges, dtype=torch.bool)\n",
        "                else:\n",
        "                    train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "                    test_mask  = torch.ones(num_edges, dtype=torch.bool)\n",
        "\n",
        "            # WEEK-SPLIT (edge-level)\n",
        "            elif self.mode == 'week_split':\n",
        "                # weeks 1–3 = days 1–21 → train\n",
        "                # week 4    = days ≥22   → test\n",
        "                train_mask = torch.tensor([day <= 21]*num_edges, dtype=torch.bool)\n",
        "                test_mask  = ~train_mask\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"mode must be 'month_split' or 'week_split'\")\n",
        "\n",
        "            data.train_edge_mask = train_mask\n",
        "            data.test_edge_mask  = test_mask\n",
        "            new_list.append(data)\n",
        "\n",
        "        self.data, self.slices = self.collate(new_list)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self): return []\n",
        "    @property\n",
        "    def processed_file_names(self): return ['flight_graphs.pt']\n",
        "    def process(self): pass"
      ],
      "metadata": {
        "id": "rlX02EBUQb8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_location = '/content/drive/Shareddrives/CS_224W_Project/data/data/skynet_clean_graphs'\n",
        "dataset = SkyNetDataset(\n",
        "    root=dataset_location,\n",
        "    mode='month_split'\n",
        ")"
      ],
      "metadata": {
        "id": "5ivBZ73oQoau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split graphs by month: Jan–Sep -> train, Oct–Dec -> test.\n",
        "# Each graph corresponds to a 6-hour window, so this enforces\n",
        "# a *temporal* train/test split rather than randomizing.\n",
        "train_graphs = [g for g in dataset if g.time.month <= 9]   # Jan–Sep\n",
        "test_graphs  = [g for g in dataset if g.time.month >= 10]  # Oct–Dec\n",
        "\n",
        "def merge_graphs(graph_list):\n",
        "    \"\"\"\n",
        "    Merge multiple PyG Data objects into a single large graph.\n",
        "\n",
        "    We do this so the GCN trains on one big connected component rather\n",
        "    than many tiny ones. Edge indices must be shifted so that nodes\n",
        "    from consecutive graphs don't overlap.\n",
        "    \"\"\"\n",
        "    x_list = []\n",
        "    ei_list = []\n",
        "    ea_list = []\n",
        "    y_list = []\n",
        "\n",
        "    cumulative_nodes = 0\n",
        "\n",
        "    for g in graph_list:\n",
        "        N = g.num_nodes\n",
        "\n",
        "        # append node features\n",
        "        x_list.append(g.x)\n",
        "\n",
        "        # shift edge indices by cumulative node count\n",
        "        ei_list.append(g.edge_index + cumulative_nodes)\n",
        "\n",
        "        # append edges + labels\n",
        "        ea_list.append(g.edge_attr)\n",
        "        y_list.append(g.y)\n",
        "\n",
        "        cumulative_nodes += N\n",
        "\n",
        "    # concatenate all\n",
        "    x = torch.cat(x_list, dim=0)\n",
        "    edge_index = torch.cat(ei_list, dim=1)\n",
        "    edge_attr  = torch.cat(ea_list, dim=0)\n",
        "    y          = torch.cat(y_list, dim=0)\n",
        "\n",
        "    # masks (all train for train graph, all test for test graph)\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    data = Data(\n",
        "        x=x,\n",
        "        edge_index=edge_index,\n",
        "        edge_attr=edge_attr,\n",
        "        y=y,\n",
        "        train_edge_mask=torch.ones(num_edges, dtype=torch.bool),\n",
        "        test_edge_mask=torch.zeros(num_edges, dtype=torch.bool)\n",
        "    )\n",
        "    data.num_nodes = x.size(0)\n",
        "    return data"
      ],
      "metadata": {
        "id": "OTVmh4KdQtBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_temporal_features(graph):\n",
        "    \"\"\"\n",
        "    Add 6 temporal features to edge_attr based on graph.time\n",
        "\n",
        "    Input: graph with edge_attr [num_edges, 8]\n",
        "    Output: graph with edge_attr [num_edges, 14]\n",
        "    \"\"\"\n",
        "    timestamp = graph.time\n",
        "    num_edges = graph.edge_attr.size(0)\n",
        "\n",
        "    # Extract temporal features from timestamp\n",
        "    dow = timestamp.weekday()\n",
        "    dow_sin = np.sin(2 * np.pi * dow / 7)\n",
        "    dow_cos = np.cos(2 * np.pi * dow / 7)\n",
        "\n",
        "    hour = timestamp.hour\n",
        "    hour_sin = np.sin(2 * np.pi * hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "    month = timestamp.month\n",
        "    month_sin = np.sin(2 * np.pi * month / 12)\n",
        "    month_cos = np.cos(2 * np.pi * month / 12)\n",
        "\n",
        "    # Create temporal feature tensor (repeat for all edges in this graph)\n",
        "    temporal_features = torch.tensor([\n",
        "        dow_sin, dow_cos,\n",
        "        hour_sin, hour_cos,\n",
        "        month_sin, month_cos\n",
        "    ], dtype=torch.float).unsqueeze(0).repeat(num_edges, 1)\n",
        "\n",
        "    # Concatenate with existing edge features\n",
        "    graph.edge_attr = torch.cat([graph.edge_attr, temporal_features], dim=1)\n",
        "\n",
        "    return graph"
      ],
      "metadata": {
        "id": "MwNRGTihQ2tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "train_graphs = [g for g in dataset if g.time.month <= 9]\n",
        "test_graphs = [g for g in dataset if g.time.month >= 10]\n",
        "\n",
        "# Add temporal features to all graphs\n",
        "train_graphs = [add_temporal_features(g) for g in train_graphs]\n",
        "test_graphs = [add_temporal_features(g) for g in test_graphs]\n",
        "\n",
        "print(f\"New edge features: {train_graphs[0].edge_attr.shape}\")  # Should be [num_edges, 14]\n",
        "\n",
        "# Normalize: both x and y features\n",
        "all_train_x = torch.cat([g.x for g in train_graphs], dim=0)\n",
        "x_mean = all_train_x.mean(dim=0, keepdim=True)\n",
        "x_std = all_train_x.std(dim=0, keepdim=True) + 1e-8\n",
        "\n",
        "all_train_edge = torch.cat([g.edge_attr for g in train_graphs], dim=0)\n",
        "edge_mean = all_train_edge.mean(dim=0, keepdim=True)\n",
        "edge_std = all_train_edge.std(dim=0, keepdim=True) + 1e-8\n",
        "\n",
        "all_train_y = torch.cat([g.y for g in train_graphs])\n",
        "y_mean = all_train_y.mean()\n",
        "y_std = all_train_y.std()\n",
        "\n",
        "for g in train_graphs + test_graphs:\n",
        "    g.x = (g.x - x_mean) / x_std\n",
        "    g.edge_attr = (g.edge_attr - edge_mean) / edge_std\n",
        "    g.y = (g.y - y_mean) / y_std\n",
        "\n",
        "print(f\"Y: mean={y_mean:.2f}, std={y_std:.2f}\")\n",
        "\n",
        "# Merge graphs\n",
        "print(\"Merging graphs...\")\n",
        "train_graph = merge_graphs(train_graphs)\n",
        "test_graph = merge_graphs(test_graphs)\n",
        "\n",
        "test_graph.train_edge_mask[:] = False\n",
        "test_graph.test_edge_mask[:] = True\n",
        "\n",
        "print(f\"Train graph: {train_graph}\")\n",
        "print(f\"Test graph: {test_graph}\")\n",
        "\n",
        "train_loader = DataLoader([train_graph], batch_size=1)\n",
        "test_loader = DataLoader([test_graph], batch_size=1)"
      ],
      "metadata": {
        "id": "cwqQmbcBQ7JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "Same gcn used for main tests"
      ],
      "metadata": {
        "id": "Fyi6MvqQQwPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# GCN model with BN + Dropout + Edge MLP (kept simple)\n",
        "# ===========================================================\n",
        "class EdgeRegressionGCN3(nn.Module):\n",
        "    def __init__(self, node_in, edge_in, hid=128, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.conv1 = GCNConv(node_in, hid)\n",
        "        self.conv2 = GCNConv(hid, hid)\n",
        "        self.conv3 = GCNConv(hid, hid)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(hid)\n",
        "        self.bn2 = nn.BatchNorm1d(hid)\n",
        "        self.bn3 = nn.BatchNorm1d(hid)\n",
        "\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(hid*2 + edge_in, hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hid, hid//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hid//2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        z = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
        "\n",
        "        z = F.relu(self.bn2(self.conv2(z, edge_index)))\n",
        "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
        "\n",
        "        z = F.relu(self.bn3(self.conv3(z, edge_index)))\n",
        "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
        "\n",
        "        src, dst = edge_index\n",
        "        e = torch.cat([z[src], z[dst], edge_attr], dim=1)\n",
        "        return self.edge_mlp(e).squeeze(-1)"
      ],
      "metadata": {
        "id": "XI_lzhuDQ0v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model + optimizer\n",
        "model = EdgeRegressionGCN3(\n",
        "    node_in=train_graph.x.size(1),\n",
        "    edge_in=train_graph.edge_attr.size(1),\n",
        "    hid=256,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "0DbvFO_nRHeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate\n",
        "trained_model, loss_history = train(model, train_loader, optimizer, criterion, epochs=70)\n",
        "results = evaluate(trained_model, test_loader)\n",
        "\n",
        "print(\"\\n=== FINAL TEST RESULTS ===\")\n",
        "print(\"RMSE:\", results[\"rmse\"])\n",
        "print(\"MAE :\", results[\"mae\"])"
      ],
      "metadata": {
        "id": "WiTeLHmCRH0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCN + memory"
      ],
      "metadata": {
        "id": "KiWzTpZtRKL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "hyPWxW4uRgKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_graphs = [g for g in dataset if g.time.month <= 9]   # Jan-Sep\n",
        "test_graphs = [g for g in dataset if g.time.month >= 10]   # Oct-Dec\n",
        "train_graphs = sorted(train_graphs, key=lambda g: g.time)\n",
        "test_graphs = sorted(test_graphs, key=lambda g: g.time)\n",
        "\n",
        "# NORMALIZE\n",
        "# Node features\n",
        "all_train_x = torch.cat([g.x for g in train_graphs], dim=0)\n",
        "x_mean = all_train_x.mean(dim=0, keepdim=True)\n",
        "x_std = all_train_x.std(dim=0, keepdim=True) + 1e-8\n",
        "\n",
        "for g in train_graphs + test_graphs:\n",
        "    g.x = (g.x - x_mean) / x_std\n",
        "\n",
        "# Edge features\n",
        "all_train_edge = torch.cat([g.edge_attr for g in train_graphs], dim=0)\n",
        "edge_mean = all_train_edge.mean(dim=0, keepdim=True)\n",
        "edge_std = all_train_edge.std(dim=0, keepdim=True) + 1e-8\n",
        "\n",
        "for g in train_graphs + test_graphs:\n",
        "    g.edge_attr = (g.edge_attr - edge_mean) / edge_std\n",
        "\n",
        "# Targets\n",
        "all_train_y = torch.cat([g.y for g in train_graphs])\n",
        "y_mean = all_train_y.mean()\n",
        "y_std = all_train_y.std()\n",
        "\n",
        "for g in train_graphs + test_graphs:\n",
        "    g.y = (g.y - y_mean) / y_std\n",
        "\n",
        "print(f\"Y normalization: mean={y_mean:.2f}, std={y_std:.2f}\")\n",
        "\n",
        "train_graph = merge_graphs(train_graphs)\n",
        "test_graph = merge_graphs(test_graphs)\n",
        "\n",
        "train_loader = DataLoader([train_graph], batch_size=1)\n",
        "test_loader = DataLoader([test_graph], batch_size=1)\n"
      ],
      "metadata": {
        "id": "SDWPjN9QRhyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "Sm8ZsSP2RN26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# GCN model with BN + Dropout + Edge MLP\n",
        "# Added memory and different edges for message passing vs loss\n",
        "# ===========================================================\n",
        "class EdgeRegressionGCNWithMemory(nn.Module):\n",
        "    def __init__(self, node_in, edge_in, num_airports, hid=128, dropout=0.3, memory_dim=64):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.memory_dim = memory_dim\n",
        "        self.num_airports = num_airports\n",
        "\n",
        "        # Airport memory (persistent state)\n",
        "        self.memory = nn.Parameter(torch.zeros(num_airports, memory_dim))\n",
        "\n",
        "        # GCN layers (now take node features + memory)\n",
        "        self.conv1 = GCNConv(node_in + memory_dim, hid)\n",
        "        self.conv2 = GCNConv(hid, hid)\n",
        "        self.conv3 = GCNConv(hid, hid)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(hid)\n",
        "        self.bn2 = nn.BatchNorm1d(hid)\n",
        "        self.bn3 = nn.BatchNorm1d(hid)\n",
        "\n",
        "        # Edge prediction MLP\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(hid*2 + edge_in, hid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hid, hid//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hid//2, 1)\n",
        "        )\n",
        "\n",
        "        # Project node embeddings to memory dimension\n",
        "        self.z_to_mem = nn.Linear(hid, memory_dim)\n",
        "\n",
        "        # Memory update GRU\n",
        "        self.memory_update = nn.GRUCell(memory_dim, memory_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, airport_ids):\n",
        "        node_memory = self.memory[airport_ids]\n",
        "        x_with_mem = torch.cat([x, node_memory], dim=1)\n",
        "\n",
        "        # GCN layers\n",
        "        z = F.relu(self.bn1(self.conv1(x_with_mem, edge_index)))\n",
        "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
        "\n",
        "        z = F.relu(self.bn2(self.conv2(z, edge_index)))\n",
        "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
        "\n",
        "        z = F.relu(self.bn3(self.conv3(z, edge_index)))\n",
        "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Edge prediction\n",
        "        src, dst = edge_index\n",
        "        e = torch.cat([z[src], z[dst], edge_attr], dim=1)\n",
        "        pred = self.edge_mlp(e).squeeze(-1)\n",
        "\n",
        "        # Update memory (no gradients, after prediction)\n",
        "        with torch.no_grad():\n",
        "            z_proj = self.z_to_mem(z)  # [num_nodes, memory_dim]\n",
        "\n",
        "            new_memory = torch.zeros_like(self.memory)\n",
        "            counts = torch.zeros(self.num_airports, device=x.device)\n",
        "\n",
        "            new_memory.index_add_(0, airport_ids, z_proj)\n",
        "            counts.index_add_(0, airport_ids, torch.ones(len(airport_ids), device=x.device))\n",
        "\n",
        "            new_memory = new_memory / (counts.unsqueeze(1) + 1e-8)\n",
        "            self.memory.data = self.memory_update(new_memory, self.memory)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def reset_memory(self):\n",
        "        self.memory.data.zero_()"
      ],
      "metadata": {
        "id": "vYnWSXAiRLD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_memory(model, loader, optimizer, criterion, epochs=40):\n",
        "    model.train()\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.reset_memory()  # Reset at start of epoch\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.airport_ids)\n",
        "            loss = criterion(pred[batch.train_edge_mask], batch.y[batch.train_edge_mask])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        loss_history.append(total_loss)\n",
        "        print(f\"Epoch {epoch+1:03d} | Loss = {total_loss:.4f}\")\n",
        "\n",
        "    return model, loss_history\n"
      ],
      "metadata": {
        "id": "RkHrC74kRWPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Evaluation function\n",
        "# ===========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate_with_memory(model, loader, y_mean, y_std):\n",
        "    model.eval()\n",
        "    model.reset_memory()  # Reset memory for test set\n",
        "\n",
        "    total_mse = 0\n",
        "    total_mae = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        pred_norm = model(batch.x, batch.edge_index, batch.edge_attr, batch.airport_ids)\n",
        "\n",
        "        # Denormalize\n",
        "        pred = pred_norm * y_std + y_mean\n",
        "        target = batch.y * y_std + y_mean\n",
        "\n",
        "        mask = batch.test_edge_mask\n",
        "        p = pred[mask]\n",
        "        t = target[mask]\n",
        "\n",
        "        total_mse += F.mse_loss(p, t, reduction='sum').item()\n",
        "        total_mae += F.l1_loss(p, t, reduction='sum').item()\n",
        "        total_count += t.numel()\n",
        "\n",
        "    return {\n",
        "        \"mse\": total_mse / total_count,\n",
        "        \"rmse\": (total_mse / total_count) ** 0.5,\n",
        "        \"mae\": total_mae / total_count\n",
        "    }\n"
      ],
      "metadata": {
        "id": "xdY5WaxcRXi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_airports = train_graphs[0].num_nodes\n",
        "print(f\"\\nNumber of airports: {num_airports}\")\n",
        "\n",
        "model = EdgeRegressionGCNWithMemory(\n",
        "    node_in=train_graph.x.size(1),\n",
        "    edge_in=train_graph.edge_attr.size(1),\n",
        "    num_airports=num_airports,\n",
        "    hid=256,\n",
        "    dropout=0.3,\n",
        "    memory_dim=64\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cMenH7AtRZwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, loss_history = train_with_memory(model, train_loader, optimizer, criterion, epochs=100)"
      ],
      "metadata": {
        "id": "KIrDSi_XR-FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_with_memory(model, test_loader, y_mean, y_std)\n",
        "print(\"\\n=== FINAL TEST RESULTS ===\")\n",
        "print(f\"RMSE: {results['rmse']:.2f} minutes\")\n",
        "print(f\"MAE : {results['mae']:.2f} minutes\")"
      ],
      "metadata": {
        "id": "Z1cjOH7GSL5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}